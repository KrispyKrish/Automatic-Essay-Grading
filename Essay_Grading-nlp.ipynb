{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install nltk pyspellchecker tensorflow numpy pandas autocorrect\n",
        "!pip install --upgrade --no-cache-dir nltk\n",
        "!pip install --upgrade --force-reinstall nltk\n"
      ],
      "metadata": {
        "id": "lshwCvLnuN9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import required libraries\n",
        "import nltk\n",
        "import statistics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from spellchecker import SpellChecker\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "# Set dataset path and load data\n",
        "DATASET_PATH = '/content/dataset.csv'"
      ],
      "metadata": {
        "id": "wya0zya9ueBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEJNfb2HuICy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "X1 = pd.read_csv(DATASET_PATH, encoding='ISO-8859-1')\n",
        "\n",
        "# Replace 'domain1_score' with the correct target column if known# For now, this example assumes there's a placeholder column called 'score' for demonstration\n",
        "# Replace 'score' with the actual target column name in your dataset\n",
        "y1 = X1['Overall']  # Update 'score' to your actual target column\n",
        "\n",
        "# Feature Engineering Functions\n",
        "def char_count(data):\n",
        "    return len(data.lower().replace(' ',''))\n",
        "\n",
        "def word_count(data):\n",
        "    return len(nltk.word_tokenize(data))\n",
        "\n",
        "def sent_count(data):\n",
        "    return len(nltk.tokenize.sent_tokenize(data))\n",
        "\n",
        "def avg_word_sentence(data):\n",
        "    words_per_sentence = [len(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(data)]\n",
        "    return np.mean(words_per_sentence)\n",
        "\n",
        "def avg_length(words):\n",
        "    words_list = nltk.word_tokenize(words)\n",
        "    return sum(len(word) for word in words_list) / len(words_list)\n",
        "\n",
        "def spelling_errors(data):\n",
        "    spell = SpellChecker()\n",
        "    words = nltk.word_tokenize(data)\n",
        "    misspelled = spell.unknown(words)\n",
        "    return len(misspelled)\n",
        "\n",
        "# POS Tagging for Nouns, Verbs, Adjectives, Adverbs\n",
        "def pos_tags(data):\n",
        "    tokens = nltk.word_tokenize(data)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
        "    adj_count = sum(1 for word, pos in pos_tags if pos.startswith('J'))\n",
        "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
        "    adv_count = sum(1 for word, pos in pos_tags if pos.startswith('R'))\n",
        "    return noun_count, adj_count, verb_count, adv_count\n",
        "\n",
        "# Applying feature engineering to the dataset\n",
        "X1['num_chars'] = X1['Essay'].apply(char_count)\n",
        "X1['num_words'] = X1['Essay'].apply(word_count)\n",
        "X1['num_sents'] = X1['Essay'].apply(sent_count)\n",
        "X1['avg_word_length'] = X1['Essay'].apply(avg_length)\n",
        "X1['avg_word_sent'] = X1['Essay'].apply(avg_word_sentence)\n",
        "X1['spelling_errors'] = X1['Essay'].apply(spelling_errors)\n",
        "X1['noun_count'], X1['adj_count'], X1['verb_count'], X1['adv_count'] = zip(*X1['Essay'].map(pos_tags))\n"
      ]
    }
  ]
}